# Chef üç≥

## Quickstart

To run examples:

1. Install dependencies

```bash
bun install
```

2. Add OpenAI Key to `.env` file:

```
OPENAI_API_KEY=sk***********Eyg
```

3. Run examples

```bash
bun run ./examples/hitl
```

or

```bash
bun run ./examples/script-generator
```

This project was created using `bun init` in bun v1.3.0. [Bun](https://bun.com) is a fast all-in-one JavaScript runtime.

## Overview

Composable context assembly for LLMs and agents

Chef is a tiny, ruthless context assembly engine.

It gives you:
‚Ä¢ Declarative ingredients instead of hand-wired pipes
‚Ä¢ Deterministic output you can reason about and test
‚Ä¢ Token-aware budgeting and graceful compression
‚Ä¢ Explainability + provenance (‚Äúwhy did this end up in the prompt?‚Äù)
‚Ä¢ Zero-boilerplate reuse of context across tools, agents, and turns

Chef‚Äôs job: take everything your agent could say, do, or remember ‚Äî history, system directives, summaries, retrieved docs, tool results, user profile, etc. ‚Äî and plate only what matters right now in the exact shape you want.

You stop duct-taping context together.
Chef does it for you.

## Why Chef exists

LLM apps all eventually hit the same wall: 1. Context is state.
You need conversation history, tool outputs, knowledge, and policy in every turn ‚Äî but not all of it, and not always in the same form. 2. Token budgets are real.
You can‚Äôt keep throwing ‚Äúthe entire convo so far‚Äù and ‚Äúall retrieved docs‚Äù at the model. Latency balloons. Cost spikes. Reasoning quality drops because the model is drowning. 3. Context gets messy fast.
You start with prompt = ....
Then an includeIf(...).
Then ‚Äúsummarize if > 1k tokens‚Äù.
Then ‚Äúprioritize tool results over memory unless‚Ä¶‚Äù
Suddenly, shipping changes to context is scarier than shipping model updates.

Chef solves that.

Chef treats context as a set of named dishes (tokens) prepared by recipes, each with:
‚Ä¢ Dependencies on other context
‚Ä¢ Optional ‚Äúdetail‚Äù levels (full vs summary)
‚Ä¢ Priority
‚Ä¢ Compression strategies
‚Ä¢ Explainability metadata

Then you tell Chef what you actually want plated this turn, give it a token budget, and it does the rest.

You get:
‚Ä¢ Reusable context components
‚Ä¢ Predictable shaping and layering
‚Ä¢ Observability of ‚Äúwhat went in‚Äù and ‚Äúwhy‚Äù

## Core mental model

### Pantry

Stuff you already have available at runtime.
Example: sessionId, user profile fields, request metadata, live tool results, etc.

You inject these values when you create a Chef instance.

```typescript
const chef = new Chef({
  sessionId: "1234",
  // userProfile: { ... },
  // searchResults: async () => fetchSearchResults("foo"),
});
```

Pantry values can be:
‚Ä¢ Raw values
‚Ä¢ Or async providers (() => Promise<T>)

Chef will treat pantry entries as first-class ‚Äúingredients‚Äù that other logic can depend on.

### Recipes

A Recipe is how you convert ingredients into actual prompt text or structured context.
‚Ä¢ It‚Äôs a class.
‚Ä¢ It declares which inputs it needs via @ingredient(...).
‚Ä¢ It returns whatever ‚Äúplate-ready‚Äù content you want.

Recipe output can be:
‚Ä¢ A string block you want in the prompt,
‚Ä¢ A JSON object,
‚Ä¢ A list of bullets,
‚Ä¢ Anything serializable.

Chef takes care of resolving the dependencies in the right order.

Recipes register into a global cookbook, so you can later ask for them by name.

(You‚Äôll see this in practice below.)

### Tokens

Every Recipe you register (or pantry value you provide) gets a token name like "ConversationHistory" or "SystemDirective".

Your agent‚Äôs prompt is just:

‚ÄúPlease give me these tokens in this order, under this budget.‚Äù

That‚Äôs literally what chef.cook() does.

### Cooking

This is the main step.

```typescript
const plated = await chef.cook({
  order: ["SystemDirective", "ConversationHistory"], // what you want
  budget: 1000, // token budget for all of it
  countTokens, // your tokenizer
  explain: true, // ask for provenance / accounting
});
```

Chef will: 1. Gather and render each requested token (‚Äúprepare the dishes‚Äù) 2. Measure token cost for each 3. If there‚Äôs a budget, it will:
‚Ä¢ Always include the first item, even if it‚Äôs huge (critical guardrail)
‚Ä¢ Sort other items by priority and try to fit them
‚Ä¢ Auto-fallback to ‚Äúcompressed‚Äù versions if available
‚Ä¢ Drop low-priority items if still over budget 4. Return:
‚Ä¢ The final prompt context (plated.context)
‚Ä¢ A full breakdown of what was included/excluded and why (plated.plates)
‚Ä¢ Token totals

## What makes Chef different

### 1. Declarative dependencies between context blocks

Recipes say what they need using @ingredient("TokenName").
You don‚Äôt write glue code every turn. You don‚Äôt manually thread state around.

Chef automatically:
‚Ä¢ resolves dependencies
‚Ä¢ caches
‚Ä¢ injects subfields (via JSONPath)
‚Ä¢ throws loudly when a contract breaks

You get strong, testable contracts between ‚Äúcontext producers.‚Äù

### 2. Token-aware budgeting and graceful fallback

You tell Chef your target budget.
Chef will:
‚Ä¢ Rank items by priority
‚Ä¢ Keep high-priority context
‚Ä¢ Prefer compressed summaries where possible
‚Ä¢ Drop low-importance stuff if you‚Äôre still over

This is per-request, not a one-time prompt hack.
You can tune this per tool, per agent, per turn.

### 3. Explainability

When you run with { explain: true }, you get introspection for free.

Every plated item comes with:
‚Ä¢ decision: included / compressed / dropped
‚Ä¢ reason
‚Ä¢ priorityScore
‚Ä¢ cost in tokens
‚Ä¢ running totals
‚Ä¢ full lineage (‚Äúthis summary was generated from ConversationHistory via @ingredient(‚Ä¶)‚Äù)

This is huge for:
‚Ä¢ debugging
‚Ä¢ audits / safety reviews
‚Ä¢ cost analysis
‚Ä¢ telling the next engineer ‚Äúdon‚Äôt worry, here‚Äôs exactly what went into the model‚Äù

### 4. Detail levels + compression hooks

A Recipe can publish multiple ‚Äúdetail profiles‚Äù like:
‚Ä¢ "full" ‚Üí full transcript
‚Ä¢ "summary" ‚Üí distilled bullets
‚Ä¢ "bullets" ‚Üí outline only

Callers choose intent:

order: [
{ token: "ConversationHistory", detail: "summary" },
"SystemDirective",
]

Chef will try to respect those detail levels, then optionally compress further if the budget still hurts.

This gives you graceful ‚Äúzoom out / zoom in‚Äù behavior without rewriting your prompt builder every week.

### 5. No tokenizer lock-in

Chef does not assume a specific model tokenizer.

You hand it countTokens(text) ‚Äî for example, a function that wraps js-tiktoken (shown in the examples below).
That means you can target gpt-4o today, local Llama tomorrow, Gemini next week, etc., without rewriting the budgeting logic.

## Example: Human-in-the-loop (HITL) agent

Let‚Äôs look at the included HITL example.
This is a minimal ‚Äúchat with memory‚Äù loop that: 1. Appends user/assistant messages to a conversation log. 2. Uses Chef to build a safe, structured prompt with:
‚Ä¢ A system directive (behavioral policy)
‚Ä¢ Recent conversation history 3. Invokes an LLM with that plated context. 4. Saves the AI response back to the conversation log.

1. Token counting

```typescript
// examples/utils/tokens.ts
import {
  getEncoding,
  getEncodingNameForModel,
  type TiktokenModel,
} from "js-tiktoken";

import { MODEL_ID } from "@examples/utils/constants";

export const encoding = getEncoding(
  getEncodingNameForModel(MODEL_ID as TiktokenModel)
);

// model-aware token counter
export const countTokens = (text: string): number => {
  return encoding.encode(text).length;
};
```

This is the tokenizer we‚Äôll pass to chef.cook() so Chef can make budget decisions based on the actual model you‚Äôre using.

2. Create your Chef instance with pantry data

```typescript
import Chef from "@lib/core/context/Chef";
import type { HITLInputType } from "@examples/hitl/specs/hitlInput";

const chef = new Chef<HITLInputType>({
  sessionId, // pantry value: can be sync value or async provider
});
```

Here the pantry only includes sessionId.
Recipes can @ingredient(‚ÄúsessionId‚Äù) to find the right conversation log for that user/session.

3. Cook the context

```typescript
const plated = await chef.cook({
  order: ["SystemDirective", "ConversationHistory"],
  budget: 1000,
  explain: true,
  countTokens,
});
```

‚Ä¢ order defines what we want ‚Äúon the plate,‚Äù and in what order.
‚Ä¢ First item is SystemDirective (i.e. ‚Äúyou are an assistant that must‚Ä¶‚Äù)
‚Ä¢ Second is ConversationHistory
‚Ä¢ budget: 1000 means:
‚Ä¢ Try to keep it under ~1000 tokens total
‚Ä¢ Always include the first item (the directive), even if it‚Äôs huge
‚Ä¢ Prefer to compress/truncate/summarize history if needed
‚Ä¢ explain: true gives us full provenance.

4. Inspect what Chef gave us

```typescript
console.log("[plated.context]:\n", plated.context);
console.log("[plated.plates]:\n", plated.plates);
console.log("[plated.totalTokens]:\n", plated.totalTokens);
```

plated.context is now your final prompt to send to the model.

plated.plates is an array of objects like:

```typescript
[
  {
    token: "SystemDirective",
    decision: "forced-include", // or "included", "dropped"
    reason: "first item is always included (compressed to save budget)",
    servedDetail: "full",
    priorityTag: "critical",
    priorityScore: 100,
    wasCompressed: false,
    compressionNote: undefined,
    originalCost: 180,
    compressedCost: 60,
    cost: 180,
    runningTotalBefore: 0,
    runningTotalAfter: 180,
    lineage: [
      {
      token: "SystemDirective",
      providerName: "SystemDirectiveRecipe",
      deps: [...],
      },
      // ...
    ],
  },
  // ...
]
```

This gives you absolute clarity into what went into the LLM call, how big it was, and what got left out (and why).

5. Call the model with structured output

```typescript
import { HITLOutputSchema } from "@examples/hitl/specs/hitlOutput";
import { llm } from "@examples/utils/llms";

const { response } = await llm
  .withStructuredOutput(HITLOutputSchema)
  .invoke(plated.context);

console.log("AI Response:", response);
```

You‚Äôre now using Chef context as input, and validating the model‚Äôs output shape.

6. Store the AI response back into history

```typescript
import { addMessage } from "@examples/hitl/conversationHistory";

// save AI response to conversation log
await addMessage(sessionId, "assistant", response);
```

The next turn, ConversationHistory will include this assistant message.
Chef will automatically pick it up (and summarize/compress if the session gets long).

## `chef.cook()` signature

```typescript
await chef.cook({
  order: [
    "SystemDirective",
    { token: "ConversationHistory", detail: "summary" },
    // you can keep adding tokens here...
  ],
  budget: 1500, // optional, number of tokens you can afford
  countTokens: (text: string) => number, // required for budgeting
  rankPriority: (info) => number, // optional override
  explain: true, // optional; default false
});
```

Key knobs:
‚Ä¢ order
Array of tokens (strings), or { token, detail } objects.
This is you saying: ‚ÄúPlate these, in this order, for this turn.‚Äù
‚Ä¢ budget
Chef will try to keep the final assembled prompt under this token limit.
‚Ä¢ First item in order is always included
‚Ä¢ Remaining items are included by priority
‚Ä¢ Auto-compression is attempted if available
‚Ä¢ If something still doesn‚Äôt fit, it‚Äôs dropped
‚Ä¢ countTokens
You control how tokens are counted. Chef doesn‚Äôt assume the model.
‚Ä¢ rankPriority
Custom priority sorter:

```typescript
rankPriority?: (info: {
  token: string;
  recipeName: string;
  priorityTag?: string | number;
  index: number;
}) => number;
```

By default, Chef maps common tags like "critical", "high", "low", or uses numeric priority fields declared on recipes. Higher score = more important.

    ‚Ä¢	explain

true returns:

```typescript
{
  context: string; // final prompt to send to model
  totalTokens: number; // final "cost"
  budget?: number; // the budget you gave us
  plates: PlateInfo[]; // full provenance/debug info
}
```

If explain is false or omitted, cook() just returns the context string directly.

## Debuggability & safety

Chef gives you something prompt hacks never will: auditability.

You can:
‚Ä¢ Log every call to chef.cook({ explain: true })
‚Ä¢ Persist plated.plates for postmortems
‚Ä¢ Prove what the model did or did not ‚Äúsee‚Äù
‚Ä¢ Enforce internal policy like ‚ÄúSystemDirective must always be first and must always plate in full‚Äù

You can even diff runs over time:
‚Ä¢ ‚ÄúWhy did the model ignore user instructions on Oct 27, 2025?‚Äù
‚Ä¢ ‚ÄúDid we silently stop including the safety block because of budget pressure?‚Äù
‚Ä¢ ‚ÄúIs history being summarized too aggressively for high-value users?‚Äù

Chef hands you this evidence.

## When to reach for Chef

Use Chef any time you find yourself doing manual prompt stitching like:
‚Ä¢ ‚ÄúTake last 10 messages, unless too long, then summarize.‚Äù
‚Ä¢ ‚ÄúAlways prepend policy block.‚Äù
‚Ä¢ ‚ÄúOnly include tool output if we actually called the tool this turn.‚Äù
‚Ä¢ ‚ÄúMake sure we include the customer‚Äôs current plan tier and SLA.‚Äù
‚Ä¢ ‚ÄúDrop sentiment analysis unless the user is escalating.‚Äù
‚Ä¢ ‚ÄúRewrite this agent into a general-purpose assistant without leaking internal tools.‚Äù

Chef is the layer that turns all of that from ‚Äòad-hoc if/else spaghetti‚Äô into reusable, explainable, testable modules.

## TL;DR pitch

Chef = Context Engineering as a first-class runtime.
‚Ä¢ You describe what context is, not how to jam it together.
‚Ä¢ You get consistent, explainable prompts across tools/turns/agents.
‚Ä¢ You get adaptive token budgeting without rewriting your prompt every sprint.
‚Ä¢ You get observability of what the model actually saw.

Chef makes context assembly:
‚Ä¢ modular
‚Ä¢ inspectable
‚Ä¢ budget-aware
‚Ä¢ production-friendly

This is how you stop duct-taping prompts and start shipping context like an adult. üçΩÔ∏è
